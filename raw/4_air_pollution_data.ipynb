{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import scipy\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from tqdm import tqdm\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from shapely.geometry import Point\n",
    "from sklearn.impute import KNNImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = '2020-03-01'\n",
    "end_date = '2022-03-31'\n",
    "long_term_exposure_years = 5\n",
    "N_neighbors_spatial_interpolation = 3\n",
    "N_neighbors_temporal_interpolation = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for measuring long-term air pollution exposure\n",
    "long_term_start_date = (datetime.datetime.strptime(start_date, \"%Y-%m-%d\") - relativedelta(years=long_term_exposure_years)).strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_information = pd.read_csv(os.path.join('air_pollution', 'all_site_information.csv'))\n",
    "location = {}\n",
    "for idx, row in site_information.iterrows():\n",
    "    station_name = row['Site Name']\n",
    "    station_lat = row['Latitude']\n",
    "    station_lon = row['Longitude']\n",
    "    location[station_name] = (station_lat, station_lon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pollution_df(pollution_name):\n",
    "    station_start_idx = []\n",
    "    for idx, line in enumerate(open(os.path.join(\"air_pollution\", \"%s.csv\" % pollution_name))):\n",
    "        if idx == 3:\n",
    "            stations = line.strip().split(\",\")\n",
    "            for j, station in enumerate(stations):\n",
    "                if station != '':\n",
    "                    station_start_idx.append(j)\n",
    "            break\n",
    "\n",
    "    station_names = list(map(lambda x: x.replace('\"', ''), \"\".join(stations).split('\"\"')))\n",
    "\n",
    "    df = pd.read_csv(os.path.join(\"air_pollution\", \"%s.csv\" % pollution_name), skiprows=4,\n",
    "                     skipfooter=1, engine='python', na_values=\"No data\")\n",
    "\n",
    "    date = df['Date']\n",
    "\n",
    "    station_df = []\n",
    "\n",
    "    def rename_column(x):\n",
    "        if \"PM10\" in x:\n",
    "            return \"pm10\"\n",
    "        elif \"PM2.5\" in x:\n",
    "            return \"pm2_5\"\n",
    "        elif \"Ozone\" in x:\n",
    "            return \"o3\"\n",
    "        elif \"Nitrogen dioxide\" in x:\n",
    "            return \"no2\"\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "    for i in range(len(station_names)):\n",
    "        station_idx = station_start_idx[i]\n",
    "        if i == len(station_names)-1:\n",
    "            next_station_idx = -1\n",
    "        else:\n",
    "            next_station_idx = station_start_idx[i+1]\n",
    "        station_name = station_names[i]\n",
    "        one_station_data = pd.DataFrame(\n",
    "            df.iloc[:, station_idx:next_station_idx:2].values)\n",
    "        one_station_data.columns = df.columns[station_idx:next_station_idx:2]\n",
    "        one_station_data['station_name'] = station_name\n",
    "        one_station_data['date'] = date\n",
    "        one_station_data = one_station_data.rename(\n",
    "            columns=lambda x: rename_column(x))\n",
    "\n",
    "        station_df.append(one_station_data)\n",
    "\n",
    "    station_df = pd.concat(station_df, axis=0, sort=True)\n",
    "    station_df['lat'] = station_df.apply(\n",
    "        lambda row: location[row['station_name']][0], axis=1)\n",
    "    station_df['lon'] = station_df.apply(\n",
    "        lambda row: location[row['station_name']][1], axis=1)\n",
    "\n",
    "    return station_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm2_5_df = get_pollution_df(\"PM2.5\")\n",
    "pm10_df = get_pollution_df(\"PM10\")\n",
    "no2_df = get_pollution_df(\"NO2\")\n",
    "o3_df = get_pollution_df(\"O3\")\n",
    "\n",
    "df = pd.merge(pm2_5_df, pm10_df, on=[\"date\", \"station_name\", \"lat\", \"lon\"], how=\"outer\")\n",
    "df = pd.merge(df, no2_df, on=[\"date\", \"station_name\", \"lat\", \"lon\"], how=\"outer\")\n",
    "df = pd.merge(df, o3_df, on=[\"date\", \"station_name\", \"lat\", \"lon\"], how=\"outer\")\n",
    "\n",
    "df = df[['date', 'station_name', 'lat', 'lon', 'pm2_5', 'pm10', 'no2', 'o3']]\n",
    "df['coords'] = list(zip(df['lon'], df['lat']))\n",
    "df['coords'] = df['coords'].apply(Point)\n",
    "\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df = df[(df[\"date\"] >= long_term_start_date) & (df[\"date\"] <= end_date)]\n",
    "df['date'] = df['date'].dt.strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ltlas_gdf = gpd.GeoDataFrame.from_file(os.path.join(\"gis\", 'lad19.geojson'))\n",
    "ltlas_gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pollution_gdf = gpd.GeoDataFrame(df, geometry='coords', crs=4326)  # WGS84\n",
    "pollution_gdf = pollution_gdf.to_crs(ltlas_gdf.crs)\n",
    "pollution_gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spatial join\n",
    "pointInPolys = gpd.tools.sjoin(pollution_gdf, ltlas_gdf, predicate=\"within\", how='right')\n",
    "\n",
    "# remove non-UK stations\n",
    "non_UK_stations = pointInPolys[pointInPolys['district_id'].isnull()]['station_name'].unique()\n",
    "pollution_gdf = pollution_gdf[~pollution_gdf['station_name'].isin(non_UK_stations)]\n",
    "pointInPolys = pointInPolys[~pointInPolys['station_name'].isin(non_UK_stations)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pollution_ltla_gdf = pointInPolys[['date', 'station_name', 'district_id', 'district_name',\n",
    "                                   'lat', 'lon', 'district_lat', 'district_lon', 'pm2_5', 'pm10', 'no2', 'o3']]\n",
    "pollution_ltla_gdf = pollution_ltla_gdf.rename(columns={'lat': 'station_lat', 'lon': 'station_lon'})\n",
    "pollution_ltla_gdf['date'] = pd.to_datetime(pollution_ltla_gdf['date'])\n",
    "stations = pollution_ltla_gdf['station_name'].unique()\n",
    "districts = pollution_ltla_gdf['district_name'].unique()\n",
    "\n",
    "print(\"# of air pollution stations: %s\" % len(stations))\n",
    "print(\"# of districts: %s\" % len(ltlas_gdf['district_name'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for districts without stations, fill empty values\n",
    "districts_without_stations = pollution_ltla_gdf[pollution_ltla_gdf['station_name'].isnull()]\n",
    "dates = pd.date_range(start=long_term_start_date, end=end_date)\n",
    "new_data = []\n",
    "for idx, row in districts_without_stations.iterrows():\n",
    "    for date in dates:\n",
    "        new_data.append({\n",
    "            \"date\": date,\n",
    "            \"station_name\": row.station_name,\n",
    "            \"district_id\": row.district_id,\n",
    "            \"district_name\": row.district_name,\n",
    "            \"station_lat\": row.station_lat,\n",
    "            \"station_lon\": row.station_lon,\n",
    "            \"district_lat\": row.district_lat,\n",
    "            \"district_lon\": row.district_lon,\n",
    "            \"pm2_5\": row.pm2_5,\n",
    "            \"pm10\": row.pm10,\n",
    "            \"no2\": row.no2,\n",
    "            \"o3\": row.o3,\n",
    "        })\n",
    "districts_without_stations_filled_dates = pd.DataFrame.from_records(new_data)\n",
    "districts_without_stations_filled_dates.head()\n",
    "new_pollution_ltla_gdf = pd.concat([pollution_ltla_gdf[~pollution_ltla_gdf['station_name'].isnull()], districts_without_stations_filled_dates], axis=0)\n",
    "new_pollution_ltla_gdf = new_pollution_ltla_gdf.sort_values(['district_name', 'station_name', 'date'])\n",
    "\n",
    "# make sure no date is missing for each district\n",
    "for district in districts:\n",
    "    assert(len(dates) == len(new_pollution_ltla_gdf[new_pollution_ltla_gdf['district_name'] == district]['date'].unique()))\n",
    "\n",
    "# make sure no district is missing for each date\n",
    "for date in dates:\n",
    "    one_day_df = new_pollution_ltla_gdf[new_pollution_ltla_gdf['date'] == date]\n",
    "    assert(len(one_day_df['district_name'].unique()) == len(ltlas_gdf['district_name'].unique()))\n",
    "\n",
    "new_pollution_ltla_gdf = new_pollution_ltla_gdf.reset_index()\n",
    "new_pollution_ltla_gdf = new_pollution_ltla_gdf.drop('index', axis=1)\n",
    "new_pollution_ltla_gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for districts that have no air pollution monitoring station,\n",
    "# use the district's location as the station location for spatial interpolation\n",
    "new_pollution_ltla_gdf['station_lat'] = new_pollution_ltla_gdf['station_lat'].fillna(value=new_pollution_ltla_gdf['district_lat'])\n",
    "new_pollution_ltla_gdf['station_lon'] = new_pollution_ltla_gdf['station_lon'].fillna(value=new_pollution_ltla_gdf['district_lon'])\n",
    "new_pollution_ltla_gdf = new_pollution_ltla_gdf.sort_values(by=['district_name', 'station_name', 'date'])\n",
    "new_pollution_ltla_gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average daily values into LTLAs\n",
    "new_pollution_ltla_gdf = new_pollution_ltla_gdf.groupby(['date', 'district_name', 'district_id', 'district_lon', 'district_lat'])[['pm2_5', 'pm10', 'no2', 'o3']].agg('mean').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_pollution_ltla_gdf['date'] = pd.to_datetime(new_pollution_ltla_gdf['date'])\n",
    "\n",
    "air_pollution_short_term = new_pollution_ltla_gdf.copy()\n",
    "air_pollution_short_term = air_pollution_short_term[(air_pollution_short_term[\"date\"] >= start_date)\n",
    "                                                  & (air_pollution_short_term[\"date\"] <= end_date)]\n",
    "air_pollution_short_term['date'] = air_pollution_short_term['date'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "air_pollution_long_term = new_pollution_ltla_gdf.copy()\n",
    "air_pollution_long_term = air_pollution_long_term[(air_pollution_long_term[\"date\"] >= long_term_start_date)\n",
    "                                                 & (air_pollution_long_term[\"date\"] < start_date)]\n",
    "air_pollution_long_term['date'] = air_pollution_long_term['date'].dt.strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Short-term exposure\n",
    "\n",
    "# Temporal interpolation\n",
    "for district in air_pollution_short_term['district_name'].unique():\n",
    "    for pollutant in ['pm2_5', 'pm10', 'no2', 'o3']:\n",
    "        district_df = air_pollution_short_term[air_pollution_short_term['district_name'] == district].reset_index().copy()\n",
    "        pollutant_values = district_df[pollutant].interpolate(method=\"linear\", limit=N_neighbors_temporal_interpolation)\n",
    "        air_pollution_short_term.loc[(air_pollution_short_term['district_name'] == district), pollutant] = air_pollution_short_term.loc[(\n",
    "            air_pollution_short_term['district_name'] == district), pollutant].fillna(value=pollutant_values)\n",
    "\n",
    "# Spatial interpolation\n",
    "def spatial_distance(X1, X2, missing_values=None):\n",
    "    # X: 'pm2_5', 'pm10', 'no2', 'o3', 'district_lon', 'district_lat'\n",
    "    return scipy.spatial.distance.euclidean(X1[4:6], X2[4:6])\n",
    "\n",
    "unique_dates = pd.unique(air_pollution_short_term['date'])\n",
    "for date in tqdm(unique_dates, desc=\"Short-term air pollution data spatial interpolation\"):\n",
    "    pollutant_matrix = air_pollution_short_term[air_pollution_short_term['date'] == date][[\n",
    "        'pm2_5', 'pm10', 'no2', 'o3', 'district_lon', 'district_lat']]\n",
    "    imputer = KNNImputer(n_neighbors=N_neighbors_spatial_interpolation, weights='distance', metric=spatial_distance)\n",
    "    imputed_matrix = imputer.fit_transform(pollutant_matrix)\n",
    "    imputed_df = pd.DataFrame(data=imputed_matrix[:, 0:4], index=pollutant_matrix.index, columns=['pm2_5', 'pm10', 'no2', 'o3'])\n",
    "    air_pollution_short_term.loc[(air_pollution_short_term['date'] == date), ['pm2_5', 'pm10', 'no2', 'o3']] = air_pollution_short_term.loc[(\n",
    "        air_pollution_short_term['date'] == date), ['pm2_5', 'pm10', 'no2', 'o3']].fillna(value=imputed_df)\n",
    "\n",
    "air_pollution_short_term = air_pollution_short_term.drop('district_lon', axis=1)\n",
    "air_pollution_short_term = air_pollution_short_term.drop('district_lat', axis=1)\n",
    "\n",
    "air_pollution_short_term[['pm2_5', 'pm10', 'no2', 'o3']] = air_pollution_short_term[['pm2_5', 'pm10', 'no2', 'o3']].astype(float)\n",
    "air_pollution_short_term.to_csv(\"../air_pollution_short_term.csv\", float_format=\"%.1f\", na_rep=\"N/A\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Long-term exposure\n",
    "\n",
    "pollutant_variables = ['pm2_5',\t'pm10',\t'no2',\t'o3']\n",
    "air_pollution_long_term = air_pollution_long_term.groupby(\n",
    "    ['district_name', 'district_id', 'district_lon', 'district_lat'])[pollutant_variables].mean().reset_index()\n",
    "\n",
    "# Spatial interpolation\n",
    "def spatial_distance(X1, X2, missing_values=None):\n",
    "    # X: 'pm2_5', 'pm10', 'no2', 'o3', 'district_lon', 'district_lat'\n",
    "    return scipy.spatial.distance.euclidean(X1[4:6], X2[4:6])\n",
    "\n",
    "pollutant_matrix = air_pollution_long_term[['pm2_5', 'pm10', 'no2', 'o3', 'district_lon', 'district_lat']]\n",
    "imputer = KNNImputer(n_neighbors=N_neighbors_spatial_interpolation, weights='distance', metric=spatial_distance)\n",
    "imputed_matrix = imputer.fit_transform(pollutant_matrix)\n",
    "imputed_df = pd.DataFrame(data=imputed_matrix[:, 0:4], index=pollutant_matrix.index, columns=['pm2_5', 'pm10', 'no2', 'o3'])\n",
    "air_pollution_long_term.loc[:, ['pm2_5', 'pm10', 'no2', 'o3']] = air_pollution_long_term.loc[:, ['pm2_5', 'pm10', 'no2', 'o3']].fillna(value=imputed_df)\n",
    "\n",
    "air_pollution_long_term = air_pollution_long_term.drop('district_lon', axis=1)\n",
    "air_pollution_long_term = air_pollution_long_term.drop('district_lat', axis=1)\n",
    "\n",
    "air_pollution_long_term[['pm2_5', 'pm10', 'no2', 'o3']] = air_pollution_long_term[['pm2_5', 'pm10', 'no2', 'o3']].astype(float)\n",
    "air_pollution_long_term.to_csv(\"../air_pollution_long_term.csv\", float_format=\"%.1f\", na_rep=\"N/A\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "878ae0fd6e3254d26856d0042bf976ac5dcc346e75060cdd861132d9be011f18"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
