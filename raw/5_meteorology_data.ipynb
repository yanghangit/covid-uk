{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import scipy\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from shapely.geometry import Point\n",
    "from sklearn.impute import KNNImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = '2020-03-01'\n",
    "end_date = '2022-03-31'\n",
    "long_term_exposure_years = 5\n",
    "N_neighbors_spatial_interpolation = 3\n",
    "N_neighbors_temporal_interpolation = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Met data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "meo_stations = {}\n",
    "for idx, line in enumerate(open(os.path.join(\"meteorology\", \"SRCE.DATA\")).readlines()):\n",
    "    if idx % 4 == 0:\n",
    "        tokens = line.split(\",\")\n",
    "        src_id = int(tokens[0])\n",
    "        lat = tokens[-3].strip()\n",
    "        lon = tokens[-2].strip()\n",
    "        if lat != \"\" and lon != \"\":\n",
    "            meo_stations[src_id] = [float(lon), float(lat)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://artefacts.ceda.ac.uk/badc_datadocs/ukmo-midas/ukmo_guide.html\n",
    "# https://artefacts.ceda.ac.uk/badc_datadocs/ukmo-midas/WH_Table.html\n",
    "# https://artefacts.ceda.ac.uk/badc_datadocs/ukmo-midas/WD_Table.html\n",
    "# https://artefacts.ceda.ac.uk/badc_datadocs/ukmo-midas/RH_Table.html\n",
    "# https://artefacts.ceda.ac.uk/badc_datadocs/ukmo-midas/RD_Table.html\n",
    "\n",
    "with open(os.path.join(\"meteorology\", \"WH_Column_Headers.txt\")) as f:\n",
    "    meo_data_header = f.readlines()\n",
    "    meo_data_header = list(map(lambda s: s.strip(), meo_data_header[0].strip().split(\",\")))\n",
    "\n",
    "meo_2020 = pd.read_csv(os.path.join(\"meteorology\", \"midas_wxhrly_202001-202012.txt\"), na_values=[\"\", \" \"], names=meo_data_header, header=None, low_memory=False)\n",
    "meo_2021 = pd.read_csv(os.path.join(\"meteorology\", \"midas_wxhrly_202101-202112.txt\"), na_values=[\"\", \" \"], names=meo_data_header, header=None, low_memory=False)\n",
    "meo_2022 = pd.read_csv(os.path.join(\"meteorology\", \"midas_wxhrly_202201-202212.txt\"), na_values=[\"\", \" \"], names=meo_data_header, header=None, low_memory=False)\n",
    "\n",
    "meo_df = pd.concat([meo_2020, meo_2021, meo_2022], axis=0).reset_index()\n",
    "\n",
    "meo_df['OB_TIME'] = pd.to_datetime(meo_df['OB_TIME'])\n",
    "meo_df = meo_df[(meo_df[\"OB_TIME\"] >= start_date) & (meo_df[\"OB_TIME\"] <= end_date)]\n",
    "meo_df['OB_TIME'] = meo_df['OB_TIME'].dt.strftime('%Y-%m-%d')\n",
    "weather_variables = ['AIR_TEMPERATURE', 'RLTV_HUM', 'MSL_PRESSURE',\n",
    "                     'WIND_SPEED', 'VISIBILITY', 'WMO_HR_SUN_DUR', 'CLD_TTL_AMT_ID']\n",
    "meo_df = meo_df[['OB_TIME', 'SRC_ID'] + weather_variables]\n",
    "# average hourly data into daily means\n",
    "meo_df = meo_df.groupby(['OB_TIME', 'SRC_ID'])[weather_variables].mean().reset_index()\n",
    "meo_df['SRC_ID'] = meo_df['SRC_ID'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add rainfall data\n",
    "\n",
    "with open(os.path.join(\"meteorology\", \"RD_Column_Headers.csv\")) as f:\n",
    "    rainfall_data_header = f.readlines()\n",
    "    rainfall_data_header = list(\n",
    "        map(lambda s: s.strip(), rainfall_data_header[0].strip().split(\",\")))\n",
    "\n",
    "rainfall_2020 = pd.read_csv(os.path.join(\"meteorology\", \"midas_raindrnl_202001-202012.txt\"), na_values=[\"\", \" \"], names=rainfall_data_header, header=None, low_memory=False)\n",
    "rainfall_2021 = pd.read_csv(os.path.join(\"meteorology\", \"midas_raindrnl_202101-202112.txt\"), na_values=[\"\", \" \"], names=rainfall_data_header, header=None, low_memory=False)\n",
    "rainfall_2022 = pd.read_csv(os.path.join(\"meteorology\", \"midas_raindrnl_202201-202212.txt\"), na_values=[\"\", \" \"], names=rainfall_data_header, header=None, low_memory=False)\n",
    "\n",
    "rainfall_df = pd.concat([rainfall_2020, rainfall_2021, rainfall_2022], axis=0).reset_index()\n",
    "rainfall_df['OB_DATE'] = pd.to_datetime(rainfall_df['OB_DATE'])\n",
    "rainfall_df = rainfall_df[(rainfall_df[\"OB_DATE\"] >= start_date) & (rainfall_df[\"OB_DATE\"] <= end_date)]\n",
    "rainfall_df['OB_TIME'] = rainfall_df['OB_DATE'].dt.strftime('%Y-%m-%d')\n",
    "rainfall_df = rainfall_df[['OB_TIME', 'SRC_ID', 'PRCP_AMT']]\n",
    "rainfall_df.head()\n",
    "\n",
    "meo_df = pd.merge(left=meo_df, right=rainfall_df, left_on=['OB_TIME', 'SRC_ID'], right_on=['OB_TIME', 'SRC_ID'], how=\"left\")\n",
    "weather_variables = weather_variables + ['PRCP_AMT']\n",
    "meo_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meo_df['coords'] = meo_df['SRC_ID'].map(meo_stations)\n",
    "meo_df = meo_df[~meo_df['coords'].isnull()]\n",
    "meo_df['lon'] = meo_df['coords'].apply(lambda x: x[0])\n",
    "meo_df['lat'] = meo_df['coords'].apply(lambda x: x[1])\n",
    "meo_df['coords'] = meo_df['coords'].apply(Point)\n",
    "meo_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ltlas_gdf = gpd.GeoDataFrame.from_file(os.path.join(\"gis\", 'lad19.geojson'))\n",
    "ltlas_gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meo_gdf = gpd.GeoDataFrame(meo_df, geometry='coords', crs=4326)  # WGS84\n",
    "meo_gdf = meo_gdf.to_crs(ltlas_gdf.crs)\n",
    "meo_gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spatial join\n",
    "pointInPolys = gpd.tools.sjoin(meo_gdf, ltlas_gdf, predicate=\"within\", how='right')\n",
    "\n",
    "# remove non-UK stations\n",
    "non_UK_stations = pointInPolys[pointInPolys['district_id'].isnull()]['SRC_ID'].unique()\n",
    "meo_gdf = meo_gdf[~meo_gdf['SRC_ID'].isin(non_UK_stations)]\n",
    "pointInPolys = pointInPolys[~pointInPolys['SRC_ID'].isin(non_UK_stations)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meo_ltla_gdf = pointInPolys[['OB_TIME', 'SRC_ID', 'district_id',\n",
    "                             'district_name', 'lat', 'lon', 'district_lat', 'district_lon'] + weather_variables]\n",
    "meo_ltla_gdf = meo_ltla_gdf.rename(columns={'OB_TIME': 'date',\n",
    "                                            'AIR_TEMPERATURE': 'temperature',\n",
    "                                            'RLTV_HUM': 'relative_humidity',\n",
    "                                            'MSL_PRESSURE': 'pressure',\n",
    "                                            'WIND_SPEED': 'wind_speed',\n",
    "                                            'VISIBILITY': 'visibility',\n",
    "                                            'WMO_HR_SUN_DUR': 'sunshine',\n",
    "                                            'CLD_TTL_AMT_ID': 'cloud_amount',\n",
    "                                            'PRCP_AMT': 'precipitation',\n",
    "                                            'SRC_ID': 'station_id',\n",
    "                                            'lat': 'station_lat', 'lon': 'station_lon'})\n",
    "new_weather_variables = ['temperature', 'relative_humidity',\n",
    "                         'pressure', 'wind_speed', 'visibility', 'sunshine', 'cloud_amount', 'precipitation']\n",
    "meo_ltla_gdf['date'] = pd.to_datetime(meo_ltla_gdf['date'])\n",
    "stations = meo_ltla_gdf['station_id'].unique()\n",
    "districts = meo_ltla_gdf['district_name'].unique()\n",
    "meo_ltla_gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"# of meteorology stations: %s\" % len(stations))\n",
    "print(\"# of districts: %s\" % len(ltlas_gdf['district_name'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for districts without stations, fill empty values\n",
    "districts_without_stations = meo_ltla_gdf[meo_ltla_gdf['station_id'].isnull()]\n",
    "dates = pd.date_range(start=start_date, end=end_date)\n",
    "new_data = []\n",
    "for idx, row in districts_without_stations.iterrows():\n",
    "    for date in dates:\n",
    "        record = {\n",
    "            \"date\": date,\n",
    "            \"station_id\": row.station_id,\n",
    "            \"district_id\": row.district_id,\n",
    "            \"district_name\": row.district_name,\n",
    "            \"station_lat\": row.station_lat,\n",
    "            \"station_lon\": row.station_lon,\n",
    "            \"district_lat\": row.district_lat,\n",
    "            \"district_lon\": row.district_lon,\n",
    "        }\n",
    "        for weather_variable in new_weather_variables:\n",
    "            record[weather_variable] = row[weather_variable]\n",
    "        new_data.append(record)\n",
    "districts_without_stations_filled_dates = pd.DataFrame.from_records(new_data)\n",
    "new_meo_ltla_gdf = pd.concat([meo_ltla_gdf[~meo_ltla_gdf['station_id'].isnull()], districts_without_stations_filled_dates], axis=0)\n",
    "new_meo_ltla_gdf = new_meo_ltla_gdf.sort_values(['district_name', 'station_id', 'date'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = []\n",
    "for station_id in stations:\n",
    "    current_station_dates = new_meo_ltla_gdf[new_meo_ltla_gdf['station_id'] == station_id]['date'].unique()\n",
    "    if len(dates) != len(current_station_dates):\n",
    "        districts = new_meo_ltla_gdf[new_meo_ltla_gdf['station_id'] == station_id]['district_name'].unique()\n",
    "        for district_name in districts:\n",
    "            one_row = new_meo_ltla_gdf[(new_meo_ltla_gdf['station_id'] == station_id) & (\n",
    "                new_meo_ltla_gdf['district_name'] == district_name)].head(1)\n",
    "            for date in dates:\n",
    "                if date not in current_station_dates:\n",
    "                    record = {\n",
    "                        \"date\": date,\n",
    "                        \"station_id\": station_id,\n",
    "                        \"district_id\": one_row.district_id.values[0],\n",
    "                        \"district_name\": district_name,\n",
    "                        \"station_lat\": one_row.station_lat.values[0],\n",
    "                        \"station_lon\": one_row.station_lon.values[0],\n",
    "                        \"district_lat\": one_row.district_lat.values[0],\n",
    "                        \"district_lon\": one_row.district_lon.values[0],\n",
    "                    }\n",
    "                    for weather_variable in new_weather_variables:\n",
    "                        record[weather_variable] = np.nan\n",
    "                    new_data.append(record)\n",
    "missing_records_df = pd.DataFrame.from_records(new_data)\n",
    "new_meo_ltla_gdf = pd.concat([new_meo_ltla_gdf, missing_records_df], axis=0)\n",
    "new_meo_ltla_gdf = new_meo_ltla_gdf.sort_values(['district_name', 'station_id', 'date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure no date is missing for each district\n",
    "for district in districts:\n",
    "    assert(len(dates) == len(new_meo_ltla_gdf[new_meo_ltla_gdf['district_name'] == district]['date'].unique()))\n",
    "\n",
    "# make sure no district is missing for each date\n",
    "for date in dates:\n",
    "    one_day_df = new_meo_ltla_gdf[new_meo_ltla_gdf['date'] == date]\n",
    "    assert(len(one_day_df['district_name'].unique()) == len(ltlas_gdf['district_name'].unique()))\n",
    "\n",
    "new_meo_ltla_gdf = new_meo_ltla_gdf.reset_index()\n",
    "new_meo_ltla_gdf = new_meo_ltla_gdf.drop('index', axis=1)\n",
    "new_meo_ltla_gdf.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for districts that have no meteorology monitoring station,\n",
    "# use the district's location as the station location for spatial interpolation\n",
    "new_meo_ltla_gdf['station_lat'] = new_meo_ltla_gdf['station_lat'].fillna(value=new_meo_ltla_gdf['district_lat'])\n",
    "new_meo_ltla_gdf['station_lon'] = new_meo_ltla_gdf['station_lon'].fillna(value=new_meo_ltla_gdf['district_lon'])\n",
    "new_meo_ltla_gdf = new_meo_ltla_gdf.sort_values(by=['district_name', 'station_id', 'date'])\n",
    "new_meo_ltla_gdf.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average daily values into LTLAs\n",
    "new_meo_ltla_gdf['district_name'] = new_meo_ltla_gdf['district_name'].astype(str)\n",
    "new_meo_ltla_gdf['district_id'] = new_meo_ltla_gdf['district_id'].astype(str)\n",
    "agg_function = {}\n",
    "for weather_variable in new_weather_variables:\n",
    "    agg_function[weather_variable] = \"mean\"\n",
    "new_meo_ltla_gdf = new_meo_ltla_gdf.groupby(by=['date', 'district_name', 'district_id', 'district_lon', 'district_lat']).agg(\"mean\").reset_index()\n",
    "new_meo_ltla_gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporal interpolation\n",
    "for district in new_meo_ltla_gdf['district_name'].unique():\n",
    "    for weather_variable in new_weather_variables:\n",
    "        district_df = new_meo_ltla_gdf[new_meo_ltla_gdf['district_name'] == district].reset_index().copy()\n",
    "        weather_values = district_df[weather_variable].interpolate(method=\"linear\", limit=N_neighbors_temporal_interpolation)\n",
    "        new_meo_ltla_gdf.loc[(new_meo_ltla_gdf['district_name'] == district), weather_variable] = new_meo_ltla_gdf.loc[(new_meo_ltla_gdf['district_name'] == district), weather_variable].fillna(value=weather_values)\n",
    "new_meo_ltla_gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spatial interpolation\n",
    "def spatial_distance(X1, X2, missing_values=None):\n",
    "    # X: ..., 'district_lon', 'district_lat'\n",
    "    return scipy.spatial.distance.euclidean(X1[-2:], X2[-2:])\n",
    "\n",
    "\n",
    "for date in tqdm(dates, desc=\"Meteorology data spatial interpolation\"):\n",
    "    weather_matrix = new_meo_ltla_gdf[new_meo_ltla_gdf['date'] == date][new_weather_variables + ['district_lon', 'district_lat']]\n",
    "    imputer = KNNImputer(n_neighbors=N_neighbors_spatial_interpolation, weights='distance', metric=spatial_distance)\n",
    "    imputed_matrix = imputer.fit_transform(weather_matrix)\n",
    "    imputed_df = pd.DataFrame(data=imputed_matrix[:, 0:len(new_weather_variables)], index=weather_matrix.index, columns=new_weather_variables)\n",
    "    new_meo_ltla_gdf.loc[(new_meo_ltla_gdf['date'] == date), new_weather_variables] = new_meo_ltla_gdf.loc[(\n",
    "        new_meo_ltla_gdf['date'] == date), new_weather_variables].fillna(value=imputed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_meo_ltla_gdf = new_meo_ltla_gdf.drop('district_lon', axis=1)\n",
    "new_meo_ltla_gdf = new_meo_ltla_gdf.drop('district_lat', axis=1)\n",
    "\n",
    "new_meo_ltla_gdf[new_weather_variables] = new_meo_ltla_gdf[new_weather_variables].astype(float)\n",
    "new_meo_ltla_gdf['date'] = new_meo_ltla_gdf['date'].dt.strftime('%Y-%m-%d')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UV data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_ids = ['bel', 'cam', 'chi', 'gla', 'inv',\n",
    "               'lee', 'ler', 'mal', 'wey', 'mcr', 'swa', 'rdg']\n",
    "station_names = ['Belfast', 'Camborne', 'Chilton', 'Glasgow', 'Inverness', 'Leeds',\n",
    "                 'Lerwick', 'Malin Head', 'Weybourne', 'Manchester', \"Swansea\", \"Reading\"]\n",
    "# https://uk-air.defra.gov.uk/research/ozone-uv/ozone-monitoring-stations\n",
    "# https://uk-air.defra.gov.uk/networks/site-info?site_id=WEYB\n",
    "station_locations = ['54.595940, -5.826939', '50.218485, -5.327063', '51.575002, -1.317720', '55.862468, -4.344691', '57.473432, -4.193860', '53.845083, -1.614636',\n",
    "                     '60.139220, -1.185319', '55.371731, -7.339425', '52.950490, 1.122017', '53.474417, -2.233773', '51.609444, -3.984910', '51.441509, -0.937416']\n",
    "assert(len(station_ids) == len(station_locations))\n",
    "station_location_map = {}\n",
    "for station_id, station_location in zip(station_ids, station_locations):\n",
    "    station_location_map[station_id] = station_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for station_id in station_location_map:\n",
    "    for date in pd.date_range(start=start_date, end=end_date):\n",
    "        record = {\n",
    "            \"station_id\": station_id,\n",
    "            \"station_lon\": float(station_location_map[station_id].split(\",\")[1]),\n",
    "            \"station_lat\": float(station_location_map[station_id].split(\",\")[0]),\n",
    "            \"date\": date,\n",
    "        }\n",
    "        date_str = date.strftime(\"%d-%b-%y\")\n",
    "        if os.path.exists(\"meteorology/uv_index/results/%s/%s.txt\" % (station_id, date_str)):\n",
    "            with open(\"meteorology/uv_index/results/%s/%s.txt\" % (station_id, date_str)) as f:\n",
    "                text = f.read()\n",
    "                if \"404 Not Found\" in text:\n",
    "                    value = np.nan\n",
    "                else:\n",
    "                    tokens = text.split(\"=\")[1].strip().split(\"@\")[:-1]\n",
    "                    values = []\n",
    "                    for i in range(0, len(tokens)-1, 2):\n",
    "                        values.append(float(tokens[i+1]))\n",
    "                    value = np.nanmean(values)\n",
    "                    value = 0.06081034*value+0.18294901626674043\n",
    "        else:\n",
    "            value = np.nan\n",
    "        record['uv_index'] = value\n",
    "        data.append(record)\n",
    "uv_df = pd.DataFrame.from_records(data)\n",
    "uv_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ltlas_gdf = gpd.GeoDataFrame.from_file(os.path.join(\"gis\", 'lad19.geojson'))\n",
    "ltlas_gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uv_df['geometry'] = uv_df[['station_lon', 'station_lat']].apply(lambda x: [x.iloc[0], x.iloc[1]], axis=1)\n",
    "uv_df['geometry'] = uv_df['geometry'].apply(Point)\n",
    "uv_df = gpd.GeoDataFrame(uv_df, geometry='geometry', crs=4326)  # WGS84\n",
    "uv_df = uv_df.to_crs(ltlas_gdf.crs)\n",
    "uv_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spatial join\n",
    "pointInPolys = gpd.tools.sjoin(uv_df, ltlas_gdf, predicate=\"within\", how='right')\n",
    "\n",
    "# remove non-UK stations\n",
    "non_UK_stations = pointInPolys[pointInPolys['district_id'].isnull()]['station_id'].unique()\n",
    "meo_gdf = uv_df[~uv_df['station_id'].isin(non_UK_stations)]\n",
    "pointInPolys = pointInPolys[~pointInPolys['station_id'].isin(non_UK_stations)]\n",
    "\n",
    "uv_ltla_gdf = pointInPolys[['date', 'station_id', 'district_id',\n",
    "                            'district_name', 'station_lat', 'station_lon', 'district_lat', 'district_lon'] + ['uv_index']]\n",
    "uv_ltla_gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "districts_without_stations = uv_ltla_gdf[uv_ltla_gdf['station_id'].isnull()]\n",
    "dates = pd.date_range(start=start_date, end=end_date)\n",
    "new_data = []\n",
    "for idx, row in districts_without_stations.iterrows():\n",
    "    for date in dates:\n",
    "        record = {\n",
    "            \"date\": date,\n",
    "            \"station_id\": row.station_id,\n",
    "            \"district_id\": row.district_id,\n",
    "            \"district_name\": row.district_name,\n",
    "            \"station_lat\": row.station_lat,\n",
    "            \"station_lon\": row.station_lon,\n",
    "            \"district_lat\": row.district_lat,\n",
    "            \"district_lon\": row.district_lon,\n",
    "            'uv_index': row.uv_index\n",
    "        }\n",
    "        new_data.append(record)\n",
    "districts_without_stations_filled_dates = pd.DataFrame.from_records(new_data)\n",
    "uv_ltla_gdf = pd.concat([uv_ltla_gdf[~uv_ltla_gdf['station_id'].isnull()], districts_without_stations_filled_dates], axis=0)\n",
    "uv_ltla_gdf = uv_ltla_gdf.sort_values(['district_name', 'station_id', 'date'])\n",
    "uv_ltla_gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = []\n",
    "for station_id in station_ids:\n",
    "    current_station_dates = uv_ltla_gdf[uv_ltla_gdf['station_id'] == station_id]['date'].unique()\n",
    "    if len(dates) != len(current_station_dates):\n",
    "        districts = uv_ltla_gdf[uv_ltla_gdf['station_id'] == station_id]['district_name'].unique()\n",
    "        for district_name in districts:\n",
    "            one_row = uv_ltla_gdf[(uv_ltla_gdf['station_id'] == station_id) & (uv_ltla_gdf['district_name'] == district_name)].head(1)\n",
    "            for date in dates:\n",
    "                if date not in current_station_dates:\n",
    "                    record = {\n",
    "                        \"date\": date,\n",
    "                        \"station_id\": station_id,\n",
    "                        \"district_id\": one_row.district_id.values[0],\n",
    "                        \"district_name\": district_name,\n",
    "                        \"station_lat\": one_row.station_lat.values[0],\n",
    "                        \"station_lon\": one_row.station_lon.values[0],\n",
    "                        \"district_lat\": one_row.district_lat.values[0],\n",
    "                        \"district_lon\": one_row.district_lon.values[0],\n",
    "                        'uv_index': np.nan\n",
    "                    }\n",
    "                    new_data.append(record)\n",
    "missing_records_df = pd.DataFrame.from_records(new_data)\n",
    "uv_ltla_gdf = pd.concat([uv_ltla_gdf, missing_records_df], axis=0)\n",
    "uv_ltla_gdf = uv_ltla_gdf.sort_values(['district_name', 'station_id', 'date'])\n",
    "uv_ltla_gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure no date is missing for each district\n",
    "for district in districts:\n",
    "    assert(len(dates) == len(uv_ltla_gdf[uv_ltla_gdf['district_name'] == district]['date'].unique()))\n",
    "\n",
    "# make sure no district is missing for each date\n",
    "for date in dates:\n",
    "    one_day_df = uv_ltla_gdf[uv_ltla_gdf['date'] == date]\n",
    "    assert(len(one_day_df['district_name'].unique())== len(ltlas_gdf['district_name'].unique()))\n",
    "\n",
    "uv_ltla_gdf = uv_ltla_gdf.reset_index()\n",
    "uv_ltla_gdf = uv_ltla_gdf.drop('index', axis=1)\n",
    "uv_ltla_gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for districts that have no UV monitoring station,\n",
    "# use the district's location as the station location for spatial interpolation\n",
    "uv_ltla_gdf['station_lat'] = uv_ltla_gdf['station_lat'].fillna(value=uv_ltla_gdf['district_lat'])\n",
    "uv_ltla_gdf['station_lon'] = uv_ltla_gdf['station_lon'].fillna(value=uv_ltla_gdf['district_lon'])\n",
    "uv_ltla_gdf = uv_ltla_gdf.sort_values(by=['district_name', 'station_id', 'date'])\n",
    "uv_ltla_gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average daily values into LTLAs\n",
    "uv_ltla_gdf['district_name'] = uv_ltla_gdf['district_name'].astype(str)\n",
    "uv_ltla_gdf['district_id'] = uv_ltla_gdf['district_id'].astype(str)\n",
    "agg_function = {}\n",
    "agg_function['uv_index'] = \"mean\"\n",
    "uv_ltla_gdf = uv_ltla_gdf.groupby(by=['date', 'district_name', 'district_id', 'district_lon', 'district_lat']).agg(agg_function).reset_index()\n",
    "uv_ltla_gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporal interpolation\n",
    "for district in uv_ltla_gdf['district_name'].unique():\n",
    "    for weather_variable in ['uv_index']:\n",
    "        district_df = uv_ltla_gdf[uv_ltla_gdf['district_name'] == district].reset_index(\n",
    "        ).copy()\n",
    "        weather_values = district_df[weather_variable].interpolate(method=\"linear\", limit=N_neighbors_temporal_interpolation)\n",
    "        uv_ltla_gdf.loc[(uv_ltla_gdf['district_name'] == district), weather_variable] = uv_ltla_gdf.loc[(uv_ltla_gdf['district_name'] == district), weather_variable].fillna(value=weather_values)\n",
    "uv_ltla_gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spatial interpolation\n",
    "def spatial_distance(X1, X2, missing_values=None):\n",
    "    # X: ..., 'district_lon', 'district_lat'\n",
    "    return scipy.spatial.distance.euclidean(X1[-2:], X2[-2:])\n",
    "\n",
    "\n",
    "for date in tqdm(dates, desc=\"UV data spatial interpolation\"):\n",
    "    weather_matrix = uv_ltla_gdf[uv_ltla_gdf['date'] == date][['uv_index'] + ['district_lon', 'district_lat']]\n",
    "    imputer = KNNImputer(n_neighbors=N_neighbors_spatial_interpolation, weights='distance', metric=spatial_distance)\n",
    "    imputed_matrix = imputer.fit_transform(weather_matrix)\n",
    "    imputed_df = pd.DataFrame(data=imputed_matrix[:, 0:len(['uv_index'])], index=weather_matrix.index, columns=['uv_index'])\n",
    "    uv_ltla_gdf.loc[(uv_ltla_gdf['date'] == date), ['uv_index']] = uv_ltla_gdf.loc[(uv_ltla_gdf['date'] == date), ['uv_index']].fillna(value=imputed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "uv_ltla_gdf = uv_ltla_gdf.drop('district_lon', axis=1)\n",
    "uv_ltla_gdf = uv_ltla_gdf.drop('district_lat', axis=1)\n",
    "\n",
    "uv_ltla_gdf[['uv_index']] = uv_ltla_gdf[['uv_index']].astype(float)\n",
    "uv_ltla_gdf['date'] = uv_ltla_gdf['date'].dt.strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "meteorology_ltla_gdf = pd.merge(new_meo_ltla_gdf, uv_ltla_gdf, left_on=(\"date\", 'district_name', 'district_id'), right_on=(\"date\", 'district_name', 'district_id'))\n",
    "meteorology_ltla_gdf.to_csv(\"../meteorology_with_uv.csv\", float_format=\"%.1f\", na_rep=\"N/A\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "878ae0fd6e3254d26856d0042bf976ac5dcc346e75060cdd861132d9be011f18"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
